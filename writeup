1. Data Exploration
	a. Are there any data quality issues present?
        - Before running any analysis on the data, I wanted to pull all of the csv files into MySQLWorkbench and create tables out of them. There were a
		  number of issues that presented themselves while attempting to do this.

        i. Users
            - There were lots of missing values in the birth_date field, which prevented the file from importing properly. When trying to import the unedited 
			  file, only the first 3 rows imported due to the empty birth_date value in row 4. My solution to this was to use find and replace in Excel to fill 
			  all of the empty birth_date values with "0001-01-01 00:00:00.000 Z" to match the rest of the date formats and allow the file to import properly. I
			  will filter out these values later as needed.
            - There were also lots of missing values in the state, language, and gender fields. These did not prevent the file from uploading although I used
			  find and replace here as well to fill all of the empty values with "NULL" so they would be pulled in with null values instead of empty strings and
			  be easier to work with later.
        ii. Transactions
            - This file had two date fields, but they were not formatted the same way. This prevented any records from importing when trying to import the raw file. To fix this issue, I created a custom 				  format in Excel for the purchase_date field that matched the date format of the scan_date field.
            - There were several barcodes that were formatted like "7.834E+11" in Excel as opposed to the full "783399746536". I switched this column to a number format and removed the decimals to get it 			  to show the whole barcode number in each row, which will be important for proper joining to the products table later.
            - The final_quantity field had many "zero" values which I changed to "0" using find and replace for consistency and so that I could import the field as double type.
            - The final_sale field had many blank values which I filled in with 0's as well.
            - The barcode field had many empty values whick I filled in with "NULL" to make the field easier to work with later if needed.
            - The store_name field had several values with special characters (like "FRESCO Y M√ÅS") which prevented the file from importing all of its rows. I searched the special characters I found in 				  the sheet to find other store_name values that needed to be fixed (converted to normal letters) and corrected them as needed using find and replace until all of the file's rows imported 				  properly.
        iii. Products
            - Similar to the transactions file, this file had lots of values with special characters across multiple fields (category_3, category_4, manufacturer, and brand) which prevented the file from                 importing all of its rows. I solved this the same way as I did in the last file.
            - After fixing all of the special character values, I filled all of the empty values with "NULL".

        - The above were all issues that I fixed in the csv files in Excel before uploading them into MySQLWorkbench, either because the file would not upload without fixing them or because it was easier 		  to fix them in Excel prior to importing. However, there are still additional DQ issues that exist, mostly to do with null values in fields that are useful for analysis:

        i. Users
            - 3,675/100,000 birth_date values are null preventing us from determining the age of those users and thus decreasing the quality and comprehensiveness of analyses involving user age

                with t1 as (
					select
						year(birth_date) as birth_year,
						count(*) as count
					from users
					group by 1
				)
				
				select count
				from t1
				where birth_year = 1

			- 4,812/100,000 state values are null preventing us from determing the state of those users and thus decreasing the quality and comprehensiveness of analyses involving user state

				with t1 as (
					select
						state,
						count(*) as count
					from users
					group by 1
				)
				
				select count
				from t1
				where state is null

			- 30,508/100,000 language values are null preventing us from determing the language of those users and thus decreasing the quality and comprehensiveness of analyses involving user language

				with t1 as (
					select
						language,
						count(*) as count
					from users
					group by 1
				)
				
				select count
				from t1
				where language is null

			- 5,892/100,000 gender values are null preventing us from determing the gender of those users and thus decreasing the quality and comprehensiveness of analyses involving user gender

				with t1 as (
					select
						gender,
						count(*) as count
					from users
					group by 1
					order by 2 desc
				)
				
				select count
				from t1
				where gender is null

			- The gender column has multiple sets of values that should be consolidated into a single value. For example, the two values "non_binary" and "Non-Binary" mean the same thing and should be 				  consolidated for consistency/ease of use in analyses. The same goes for the values "not_listed", "My gender isn't listed", and "not_specified" as well as "Prefer not to say" and 						  "prefer_not_to_say".

				select
					gender,
				    count(*) as count
				from users
				group by 1
				order by 1

		ii. Transactions
			- The clearest issue in this table is that every single receipt_id, barcode combo has at least two rows in the dataset. This is problematic because in order to prevent potentially incorrect 				  analysis results when joining to other tables, the table needs to be cleaned before use. Each receipt_id, barcode combo should only have one row.

				select
					receipt_id,
				    barcode,
				    count(*) as count
				from txns
				group by 1,2
				order by 3

			- Related to the last point, 25,321/50,000 rows have a 0 in either the final_quantity or final_sale fields. These rows should have non-zero values in both fields if they are real transactions, 			  so >50% of this table is not usable. 159/50,000 rows have 0's in both the final_quantity and final_sale fields.

				select count(*)
				from txns
				where final_quantity = 0 or final_sale = 0;

				select count(*)
				from txns
				where final_quantity = 0 and final_sale = 0;

			- 5,762/50,000 rows have null barcodes, which is problematic because they will not be able to be used in analyses involving joins to the product table due to their not being able to join on 				  this field

				select count(*)
				from txns
				where barcode is null

		iii. Products
			- 111/845,552 rows have null values in all category fields, which potentially decreases the quality and comprehensiveness of analyses based on category. Categories 2,3, and 4 each have 					  increasing null rates, but this is not necessarily a data quality issue as it is expected that not all rows will have values for all categories

				select count(*)
				from products
				where category_1 is null

			- 226,474/845,552 rows have null values in the manufacturer field, which decreases the quality and comprehensiveness of analyses involving manufacturer

				select count(*)
				from products
				where manufacturer is null

			- 226,472/845,552 rows have null values in the brand field, which decreases the quality and comprehensiveness of analyses involving brand

				select count(*)
				from products
				where brand is null

			- 4,025/845,552 rows have null barcodes, which is problematic because they will not be able to be used in analyses involving joins to the product table due to their not being able to join on 				  this field

				select count(*)
				from products
				where barcode is null


    b. Are there any fields that are challenging to understand?
		- For the most part, no, but one thing I looked into was decimal values in the final_quantity field of the transactions table. I ran the below query to isolate these rows and see which products 		  	  they were for. I know that fractional quantities are sometimes valid (like in a cryptocurrency purchase for example), but these were mostly food items, which it doesn't really make sense to have 		  a fractional quantity of. Nevertheless, I chose not to remove these rows from my analyses in part 2 as they otherwise appear to be valid transactions.

			select *
			from txns
			join products
			using (barcode)
			where floor(final_quantity) != ceiling(final_quantity)

2. SQL Queries
	a. What are the top 5 brands by receipts scanned among users 21 and over?

		Query (run on 3/4/25):

			-- add row number column to help get best record for each receipt_id, barcode combo in next step and remove any remaining rows with invalid txn info
			with txns1 as (
				select *,
					row_number() over(partition by receipt_id, barcode order by final_quantity desc, final_sale desc) as rn
				from txns
			    where final_quantity > 0 and final_sale > 0
			)
			
			-- filter to only keep best record for each
			,txns2 as (
				select *
				from txns1
				where rn = 1
			)
			
			-- add age column and remove rows that had null dates
			,users1 as (
				select *,
					case
						when month(curdate()) < month(birth_date) 
							or (month(curdate()) = month(birth_date) and day(curdate()) < day(birth_date)) 
							then year(curdate()) - year(birth_date) - 1
						else year(curdate()) - year(birth_date)
					end as age
				from users
				where birth_date != "0001-01-01 00:00:00"
			    having age >= 21
			)
			
			-- remove rows with null brand or barcode because they will either not join or not be useful for this analysis
			,products1 as (
				select *
			    from products
			    where brand is not null and barcode is not null
			)
			
			-- join all tables, group by brand, sort by receipt count descending then brand ascending
			select brand, count(*)
			from txns2 a
			join users1 b
			on a.user_id = b.id
			join products1 c
			using (barcode)
			group by 1
			order by 2 desc, 1

		Answer:

			1. DOVE - 3 receipts
			2. NERDS CANDY - 3 receipts
			3. COCA-COLA - 2 receipts
			4. GREAT VALUE - 2 receipts
			5. HERSHEY'S - 2 receipts


	b. What are the top 5 brands by sales among users that have had their account for at least six months?

		Query:

			-- add row number column to help get best record for each receipt_id, barcode combo in next step and remove any remaining rows with invalid txn info
			with txns1 as (
				select *,
					row_number() over(partition by receipt_id, barcode order by final_quantity desc, final_sale desc) as rn
				from txns
			    where final_quantity > 0 and final_sale > 0
			)
			
			-- filter to only keep best record for each
			,txns2 as (
				select *
				from txns1
				where rn = 1
			)
			
			-- add account age over 6 months flag column
			,users1 as (
				select *,
					case when created_date <= date_sub(curdate(), interval 6 month) then 1
			        else 0
			        end as account_age_at_least_6_months
				from users
			    having account_age_at_least_6_months = 1
			)
			
			-- remove rows with null brand or barcode because they will either not join or not be useful for this analysis
			,products1 as (
				select *
			    from products
			    where brand is not null and barcode is not null
			)
			
			-- join all tables, group by brand, sort by total sales descending
			select brand, round(sum(final_sale), 2) as total_sales
			from txns2 a
			join users1 b
			on a.user_id = b.id
			join products1
			using (barcode)
			group by 1
			order by 2 desc

		Answer:

			1. CVS - 72
			2. DOVE - 30.91
			3. TRIDENT - 23.36
			4. COORS LIGHT - 17.48
			5. TRESEMME - 14.58


	c. Which is the leading brand in the Dips & Salsa category?

		Query:

			-- add row number column to help get best record for each receipt_id, barcode combo in next step and remove any remaining rows with invalid txn info
			with txns1 as (
				select *,
					row_number() over(partition by receipt_id, barcode order by final_quantity desc, final_sale desc) as rn
				from txns
			    where final_quantity > 0 and final_sale > 0
			)
			
			-- filter to only keep best record for each
			,txns2 as (
				select *
				from txns1
				where rn = 1
			)
			
			-- filter to get only rows where category 2 is "Dips & Salsa" and brand is not null
			,products1 as (
			select *
			from products
			where category_2 = "Dips & Salsa"
				and brand is not null
			)
			
			-- join txns to products, group by brand, sort by total sales descending
			select brand, round(sum(final_sale), 2) as total_sales
			from txns2
			join products1
			using (barcode)
			group by 1
			order by 2 desc;

		Answer:

			1. TOSTITOS - 181.30
			2. GOOD FOODS - 94.91
			3. PACE - 85.75
			4. FRITOS - 67.16
			5. MARKETSIDE - 65.22


3. Stakeholder Communication

Good morning all,

I am reaching out today to report the results of an investigation I did into any existing data quality issues in the user, transactions, and product tables as well as some insight involving those tables. Before getting to the insight, I'd first like to point out some important data quality issues in these datasets that need to be resolved as soon as possible in order to create more reliable and comprehensive analyses. The current state of the data leads to a very small sample size after joining the tables together and filtering out irrelevant rows. Due to this small sample size, no decisions should be made based on the insight I will discuss below at the moment.

The most glaring issue I found in the data is that the user table seems to be missing a significant number of users. Only 91/17,694 (~0.5%) users that are in the txns table are also in the users table, which leads to a very small sample size when joining these two tables and significantly decreases the reliability of the analyses generated from this data. This is a major issue that needs to be fixed immediately.

Another major issue has to do with invalid records in the transactions table. Each receipt_id, barcode combination represents 1 transaction and should have 1 row in the table. The table has 50,000 rows, but only 24,485 unique receipt_id, barcode combos meaning there are more invalid records than valid ones. This also means that the table always needs to be cleaned before use in analyses. Additionally, there are many rows in the transactions table with values of 0 in the final_quantity and/or final_sale fields. These are not valid and should be removed.

The other issues are less impactful and have to do with null values in fields that are useful for analysis. For example, there are many null values in the user table for the fields birth_date, state, language, and gender, in the transactions table for the barcode field, and in the products table for the fields category_1, category_2, category_3, category_4, manufacturer, brand and barcode. It is important that we do what we can to fill as many of these values as possible in order to ensure high quality analyses.

One trend I found interesting while looking into this data is that among the top 5 leading brands based on total sales within the "Dips & Salsa" category, Tostito's is in the lead by a large margin with the second place brand (Good Foods) having just over half of Tostito's total sales. However, as I explained above, this insight was based on a very small sample size primarily due to the user issue, and no decisions should be made based off of this yet as once this issue is fixed, it is possible that the top 5 leading brands will look completely different.

It is crucial that the data quality issues I mentioned above be resolved as soon as possible so that we can move forward with analyses confidently and ensure that we are only making business decisions based off of good data. One other thing I noticed during this investigation is that there are some decimal values in the final_quantity field in the transactions table for food products that I believe should only have whole number values. I'd like to understand if there is a reason for this or if this is an actual issue. I look forward to getting these issues resolved quickly!

Best,
Colin
